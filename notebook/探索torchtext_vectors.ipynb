{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import pandas as pd\n",
    "from torchtext.vocab import Vectors\n",
    "from torch.nn import init\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "# 注意本模型训练在单机单gpu上\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,format = '%(message)s')\n",
    "\n",
    "\n",
    "# 定义Dataset\n",
    "class GrandDataset(data.Dataset):\n",
    "    def __init__(self,path,text_field,label_field,text_type='word',test=False,aug=False,**kwargs):\n",
    "        fields = [('text',text_field),('label',label_field)]\n",
    "        examples = []\n",
    "        csv_data = pd.read_csv(path)\n",
    "        logging.info('read data from {}'.format(path))\n",
    "\n",
    "        if text_type == 'word':\n",
    "            text_type = 'word_seg'\n",
    "\n",
    "        if test:\n",
    "            # 如果为测试集，则不加载label\n",
    "            for text in csv_data[text_type]:\n",
    "                examples.append(data.Example.fromlist([text,None],fields))\n",
    "\n",
    "        else:\n",
    "            for text,label in zip(csv_data[text_type],csv_data['class']):\n",
    "                if aug:\n",
    "                    # 数据增强，包括打乱顺序和随机丢弃\n",
    "                    rate = random.random()\n",
    "                    if rate > 0.5:\n",
    "                        text = self.dropout(text)\n",
    "                    else:\n",
    "                        text = self.shuffle(text)\n",
    "                examples.append(data.Example.fromlist([text,label - 1],fields))\n",
    "        super(GrandDataset,self).__init__(examples,fields,**kwargs)\n",
    "\n",
    "    def shuffle(self,text):\n",
    "        text = np.random.permutation(text.strip().split())\n",
    "        return ' '.join(text)\n",
    "\n",
    "    def dropout(self,text,p = 0.5):\n",
    "        # random delete some text\n",
    "        text = text.strip().split()\n",
    "        text_len = len(text)\n",
    "        indexs = np.random.choice(text_len,int(text_len * p))\n",
    "        for i in indexs:\n",
    "            text[i] = ''\n",
    "        return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read data from ../data/mini/word/train_set.csv\n",
      "read data from ../data/mini/word/val_set.csv\n",
      "read data from ../data/mini/word/test_set.csv\n",
      "Loading vectors from ../data/mini/word2vec_data/word_50.txt\n",
      "100%|██████████| 71654/71654 [00:01<00:00, 54329.47it/s]\n",
      "Saving vectors to .vector_cache/word_50.txt.pt\n",
      "load word2vec vectors from ../data/mini/word2vec_data/word_50.txt\n",
      "building word vocabulary ....\n"
     ]
    }
   ],
   "source": [
    "data_dir='../data/mini/'\n",
    "text_type='word'\n",
    "embedding_dim=50\n",
    "device=0\n",
    "'''\n",
    "负责数据的生成\n",
    ":param hp:\n",
    "    hp.max_text_len\n",
    "    hp.data_dir\n",
    "    hp.text_type\n",
    "    hp.embedding_dim\n",
    "    hp.device\n",
    ":return:\n",
    "    train_iter\n",
    "    val_iter\n",
    "    test_iter\n",
    "    len(vocab)\n",
    "    vectors\n",
    "'''\n",
    "\n",
    "tokenize = lambda x:x.split()\n",
    "# text 设置fix_length\n",
    "TEXT = data.Field(sequential=True,tokenize=tokenize,batch_first=True,fix_length=50)\n",
    "LABEL = data.Field(sequential=False,batch_first=True,use_vocab=False)\n",
    "\n",
    "# load path 训练数据存储在  data_dir/text_type/下\n",
    "train_path = os.path.join(data_dir,text_type,'train_set.csv')\n",
    "val_path = os.path.join(data_dir,text_type, 'val_set.csv')\n",
    "test_path = os.path.join(data_dir, text_type, 'test_set.csv')\n",
    "\n",
    "train = GrandDataset(train_path,text_field=TEXT,label_field=LABEL,text_type=text_type,test=False)\n",
    "val = GrandDataset(val_path,text_field=TEXT,label_field=LABEL,text_type=text_type,test=False)\n",
    "test = GrandDataset(test_path,text_field=TEXT,label_field=None,text_type=text_type,test=True)\n",
    "\n",
    "cache = '.vector_cache'\n",
    "if not os.path.exists(cache):\n",
    "    os.mkdir(cache)\n",
    "\n",
    "# 词向量的位置在 data_dir/word2vec_data/ 下，名称有text_type和 embed_dim 确定\n",
    "embedding_path = os.path.join(data_dir,'word2vec_data','{}_{}.txt'.format(text_type,embedding_dim))\n",
    "vectors = Vectors(name=embedding_path,cache=cache)\n",
    "logging.info('load word2vec vectors from {}'.format(embedding_path))\n",
    "# 没有命中的token的初始化方式\n",
    "vectors.unk_init = init.xavier_uniform_\n",
    "\n",
    "# 构建vocab\n",
    "logging.info('building {} vocabulary ....'.format(text_type))\n",
    "TEXT.build_vocab(train,val,test,min_freq=5,vectors=vectors)\n",
    "\n",
    "# 构建Iterator\n",
    "# 在test_iter,val_iter ,shuffle,sort.repeat 一定要设置成False，要不然会被torchtext 搞乱样本顺序\n",
    "# 如果输入变长序列，sort_within_batch 需要设置成True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([71656, 50])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
