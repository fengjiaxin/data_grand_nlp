{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Conv1D\n",
    "\n",
    "&emsp;&emsp;一维卷积：就是卷积时只看纵列，如下图所示：\n",
    "\n",
    "![Conv!D](./pic/conv1D.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;一维卷积在卷积的时候只在纵列一个方向上滑动，上图的初始矩阵始 7 * 5,7是句子的长度，5是词向量的维度。\n",
    "\n",
    "&emsp;&emsp;卷积核有三种大小，分别是2\\*5，3\\*5，4\\*5，每种卷积核有两个。\n",
    "\n",
    "&emsp;&emsp;输出分别是 (6,1),(5,1),(4,1)\n",
    "\n",
    "&emsp;&emsp;经过一个卷积核大小为4的max_pooling，变成1个值，最后获得6个值，进行拼接，在经过一个全连接层，输出2个类别的概率。\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;一维卷积最核心的概念就是卷积核只在纵列滑动，然后卷积核的宽度核原始矩阵的宽度一致。\n",
    "\n",
    "&emsp;&emsp;Conv1d(in_channels,out_channels,kernel_size)\n",
    "\n",
    "1. input size (batch_size,c_in,l_in):\n",
    "    + 1.1 c_in:理解为卷积核滑动方向上的宽度,在上面的例子中,c_in就是词向量的宽度，5\n",
    "    + 1.2 l_in:理解为卷积核滑动方向上的长度，在上面的例子中，l_in就是句子的长度，7\n",
    "2. output size (batch_size,c_out,l_out):\n",
    "    + 2.1 c_out:理解为卷积核的个数，在上面的例子中，每种卷积核有两种，因此c_out = 2\n",
    "    + 2.2 l_out:理解为卷积核滑动之后的长度,l_out = (l-kernel_size)/stride + 1\n",
    "3. kennel_size :就是卷积核覆盖滑动范围的长度，在上面的例子中，分别为2,3,4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "\n",
    "&emsp;&emsp;举一个简单的例子,句子长度l_in = 7,词向量维度c_in = 5,batch_size = 8,kernel_size=3,out_channels = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "conv1 = nn.Conv1d(in_channels=5,out_channels=2,kernel_size = 3)\n",
    "\n",
    "# [batch_size,seq_len,embed_dim]\n",
    "input = torch.randn(8,7,5)\n",
    "\n",
    "# 交换维读 [batch_size,embed_dim,seq_len]\n",
    "input = input.permute(0,2,1)\n",
    "\n",
    "# output [batch_size,out_channels = 2,(seq_len - kernel_size + 1)/1=5]\n",
    "output = conv1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP中CNN模型常见的pooling\n",
    "\n",
    "##### Max Pooling Over Time\n",
    "\n",
    "&emsp;&emsp;是NLP中CNN模型中常见的一种下采样操作，意思是对于某个Filter抽取到若干特征值，只取其中得分最大的那个值作为Pooling层保留值，其他特征值全部抛弃。\n",
    "\n",
    "&emsp;&emsp;CNN采用Max Pooling操作有几个好处：1.可以保证特征的位置与旋转不变性，因为这个强特征无论出现在什么位置，都可以提取出来，但是对于NLP任务来说，特征的出现位置信息是很重要的，比如主语位置一般在句子头，宾语一般在句子尾等，这些信息有时候对于分类任务还是很重要的，但是max pooling 基本把这些信息扔掉了。\n",
    "\n",
    "##### K-Max Pooling Over Time\n",
    "&emsp;&emsp;k-max pooling 可以取所有特征值中得分在Top-K的值，并保留这些特征原始的先后顺序。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
